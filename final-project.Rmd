---
title: "Final Project"
author: "Riley Maher"
date: "3/28/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tm)
library(stopwords)
library(tokenizers)
library(dplyr)
library(tidytext)
library(sentimentr)
library(gmodels)
library(C50)
library("caret")
```

## Loading Data
```{r}
# News Train
news_train <- read.csv('train.csv')
news_train$id <- NULL

news_test <- read.csv('test.csv')
news_test$id <- NULL

news_test_result <- read.csv('submit.csv')

head(news_train)
```

```{r}
sentiment_train <- read.csv('train_sentiment1.csv')
head(sentiment_train)
```

```{r}
sentiment_test <- read.csv('test_sentiment1.csv')
head(sentiment_test)
```


```{r}
news_test$fake <- as.factor(news_test_result$label) #changed to factor in order to be used in confusion Matrix
```

## Data Preperation and Cleaning

### Train Data
```{r cache = TRUE}
# Cleaning article titles
news_train$cleaned_title <- tolower(news_train$title)
news_train$cleaned_title <- removePunctuation(news_train$cleaned_title)
news_train$cleaned_title <- stripWhitespace(news_train$cleaned_title)

news_train$title_character_length <- lapply(news_train$cleaned_title, nchar)#Creates variable of length of title
news_train$cleaned_title <- tokenize_words(news_train$cleaned_title, stopwords = stopwords::stopwords("en"))
news_train$cleaned_title <- lapply(news_train$cleaned_title, tokenize_word_stems)
news_train$title_word_length <- lapply(news_train$cleaned_title, length)
```


```{r cache = TRUE}
# Cleaning article text
news_train$cleaned_text <- tolower(news_train$text)
news_train$cleaned_text <- removePunctuation(news_train$cleaned_text)
news_train$cleaned_text <- stripWhitespace(news_train$cleaned_text)
news_train$text_character_length <- lapply(news_train$cleaned_text, nchar)
news_train$cleaned_text <- tokenize_words(news_train$cleaned_text, stopwords = stopwords::stopwords("en"))
news_train$cleaned_text <- lapply(news_train$cleaned_text, tokenize_word_stems)
news_train$text_word_length <- lapply(news_train$cleaned_text, length)

# write.csv(sentiment(get_sentences(paste(news_train$cleaned_text, sep='', collapse=NULL)))$sentiment, 'C:\\Users\\Riley\\Documents\\Homework\\Ross\\TO_414\\to414-final-project\\train_sentiment1.csv')

# Returns emotional sentiment of article with numeric score
news_train$text_sentiment <- sentiment_train$sentiment_score
```

```{r}
names(news_train)[names(news_train)== "label"] <- "fake" #renaming dependent variable for readability purposes
```

### Test Data
```{r cache = TRUE}
# Cleaning article titles
news_test$cleaned_title <- tolower(news_test$title)
news_test$cleaned_title <- removePunctuation(news_test$cleaned_title)
news_test$cleaned_title <- stripWhitespace(news_test$cleaned_title)

news_test$title_character_length <- lapply(news_test$cleaned_title, nchar)#Creates variable of length of title
news_test$cleaned_title <- tokenize_words(news_test$cleaned_title, stopwords = stopwords::stopwords("en"))
news_test$cleaned_title <- lapply(news_test$cleaned_title, tokenize_word_stems)
news_test$title_word_length <- lapply(news_test$cleaned_title, length)
```


```{r cache = TRUE}
# Cleaning article text
news_test$cleaned_text <- tolower(news_test$text)
news_test$cleaned_text <- removePunctuation(news_test$cleaned_text)
news_test$cleaned_text <- stripWhitespace(news_test$cleaned_text)
news_test$text_character_length <- lapply(news_test$cleaned_text, nchar)
news_test$cleaned_text <- tokenize_words(news_test$cleaned_text, stopwords = stopwords::stopwords("en"))
news_test$cleaned_text <- lapply(news_test$cleaned_text, tokenize_word_stems)
news_test$text_word_length <- lapply(news_test$cleaned_text, length)

# write.csv(sentiment(get_sentences(paste(news_test$cleaned_text, sep='', collapse=NULL)))$sentiment, 'C:\\Users\\Riley\\Documents\\Homework\\Ross\\TO_414\\to414-final-project\\test_sentiment1.csv')

# Returns emotional sentiment of article with numeric score
news_test$text_sentiment <- sentiment_test$sentiment_score
```

```{r cache = TRUE}
# Adjusting structure types for usability 
news_train$title_character_length <- as.numeric(news_train$title_character_length)
news_train$text_character_length <- as.numeric(news_train$text_character_length)
news_train$title_word_length <- as.numeric(news_train$title_word_length)
news_train$text_word_length <- as.numeric(news_train$text_word_length)
news_train$text_sentiment <- as.numeric(news_train$text_sentiment)

news_test$title_character_length <- as.numeric(news_test$title_character_length)
news_test$text_character_length <- as.numeric(news_test$text_character_length)
news_test$title_word_length <- as.numeric(news_test$title_word_length)
news_test$text_word_length <- as.numeric(news_test$text_word_length)
news_test$text_sentiment <- as.numeric(news_test$text_sentiment)

```

## Summary of Varibles
The Dependent Variable we are hoping to predict is whether or not an article is "fake" news, this is a binomial variable.

The independent variables we are using to predict fake news are: text_character_length (length of the article in characters), text_word_length (length of article in words), title_character_length (length of title in characters), title_word_length (length of title in words), sentiment (ADD EXPLINATION OF SENTIMENT)

```{r}
head(news_train)
```

```{r}
head(news_test)
```

## Linear Regression

### Model Design
```{r}
FakeLinear <- lm(fake ~ text_character_length + text_word_length + title_character_length + title_word_length, data = news_train )
summary(FakeLinear)
#Significant factors: text_character_length, text_word_length, title_character_length, title_word_length
```
```{r}
FakeLinear <- lm(fake ~ text_character_length + text_word_length + title_character_length + title_word_length + (text_character_length*text_word_length) + (title_character_length*title_word_length), data = news_train )
summary(FakeLinear)
#Significant factors: text_character_length, text_word_length, title_character_length, title_word_length, title_character_length*title_word_length
```
```{r}
FakeLinear <- lm(fake ~ text_character_length + text_word_length + title_character_length + title_word_length + (title_character_length*title_word_length) + I(text_character_length ^2) + I(title_character_length ^2) +I(text_word_length ^2) +I(title_word_length ^2), data = news_train)
summary(FakeLinear)
#Significant factors: text_character_length, text_word_length, title_character_length, title_word_length, title_character_length*title_word_length, text_character_length^2, text_word_length^2, title_character_length^2, title_word_length^2
```
WILL ADD ANALYSIS AFTER ALL FACTORS AVAIBLE (i.e. sentiment)
### Model Testing
```{r}
LinearPredict <- predict(FakeLinear, news_test)
LinearPredict <- ifelse(LinearPredict > 0.43, 1,0)#Tested multiple cutoffs, this cutoff produces highest Kappa
LinearPredict <- as.factor(LinearPredict)

confusionMatrix(news_test$fake, LinearPredict)
```

## Logistic Regression
### Model Design
```{r}
FakeLog <- glm(fake ~ text_character_length + text_word_length + title_character_length + title_word_length, data = news_train )
summary(FakeLog)
#Significant factors: text_character_length, text_word_length, title_character_length, title_word_length
```
```{r}
FakeLog <- glm(fake ~ text_character_length + text_word_length + title_character_length + title_word_length + (text_character_length*text_word_length) + (title_character_length*title_word_length), data = news_train )
summary(FakeLog)
#Significant factors: text_character_length, text_word_length, title_character_length, title_word_length, title_character_length*title_word_length
```
```{r}
FakeLog <- glm(fake ~ text_character_length + text_word_length + title_character_length + title_word_length + (title_character_length*title_word_length) + I(text_character_length ^2) + I(title_character_length ^2) +I(text_word_length ^2) +I(title_word_length ^2), data = news_train)
summary(FakeLog)
#Significant factors: text_character_length, text_word_length, title_character_length, title_word_length, title_character_length*title_word_length, text_character_length^2, text_word_length^2, title_character_length^2, title_word_length^2
```
### Model Testing
```{r}
LogPredict <- predict(FakeLog, news_test)
LogPredict <- ifelse(LogPredict > 0.43, 1,0) #Tested multiple cutoffs, this cutoff produces highest Kappa
LogPredict <- as.factor(LogPredict)

confusionMatrix(news_test$fake, LogPredict)
```


## Decision Tree

```{r}
#building decision tree model
library(C50)

news_train$fake <- as.factor(news_train$fake)

fakedecisionmodel <- C5.0(fake ~ text_character_length + text_word_length + title_character_length + title_word_length , data = news_train)

plot(fakedecisionmodel)

summary(fakedecisionmodel)
```

```{r}
#checking prediction accuracy

fakepred <- predict(fakedecisionmodel, news_test)

library(gmodels)

CrossTable(news_test$fake, fakepred)

# Confusion Matrix
confusionMatrix(as.factor(fakepred), news_test$fake, positive = '1')
# 64.27% Accuracy with 0.2284 Kappa Statistics
```

```{r}
#adding error costs for false negative (missing identifying something as fake news)

error_cost <- matrix(c(0,0,4,0), nrow = 2)

error_cost

fakecostdecisionmodel <- C5.0(fake ~ text_character_length + text_word_length + title_character_length + title_word_length , data = news_train, costs = error_cost)

```

```{r}
#checking new error cost model prediction accuracy

fakecostpred <- predict(fakecostdecisionmodel, news_test)

library(gmodels)

CrossTable(news_test$fake, fakecostpred)

#Confusion Matrix
confusionMatrix(as.factor(fakecostpred), as.factor(news_test$fake), positive = '1')
# Accuracy increases to 74.75% with improved Kappa Statistics of 0.4625
# Overall performance has been improved

```

