---
title: "Final Project"
author: "Riley Maher"
date: "3/28/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tm)
library(stopwords)
library(tokenizers)
library(dplyr)
library(tidytext)
library(sentimentr)
library(gmodels)
library(C50)
library("caret")
library('VIF')
```

## Loading Data
```{r}
# News Train
news_train <- read.csv('train.csv')
news_train$id <- NULL

news_test <- read.csv('test.csv')
news_test$id <- NULL

news_test_result <- read.csv('submit.csv')

head(news_train)
```

```{r}
sentiment_train <- read.csv('train_sentiment1.csv')
head(sentiment_train)
```

```{r}
sentiment_test <- read.csv('test_sentiment1.csv')
head(sentiment_test)
```


```{r}
news_test$fake <- as.factor(news_test_result$label) #changed to factor in order to be used in confusion Matrix
```

## Data Preperation and Cleaning

### Train Data
```{r cache = TRUE}
library(tokenizers)
library(stringi)
# Cleaning article titles
news_train$cleaned_title <- tolower(news_train$title)
news_train$cleaned_title <- removePunctuation(news_train$cleaned_title)
news_train$cleaned_title <- stripWhitespace(news_train$cleaned_title)
news_train$title_character_length <- lapply(news_train$cleaned_title, nchar)#Creates variable of length of title
news_train$cleaned_title <- tokenize_word_stems(news_train$cleaned_title, stopwords = stopwords::stopwords("en"))
news_train$cleaned_title <- lapply(news_train$cleaned_title, tokenize_word_stems)
news_train$title_word_length <- lapply(news_train$cleaned_title, length)
```


```{r cache = TRUE}
# Cleaning article text
news_train$cleaned_text <- tolower(news_train$text)
news_train$cleaned_text <- removePunctuation(news_train$cleaned_text)
news_train$cleaned_text <- stripWhitespace(news_train$cleaned_text)
news_train$text_character_length <- lapply(news_train$cleaned_text, nchar)
news_train$cleaned_text <- tokenize_word_stems(news_train$cleaned_text, stopwords = stopwords::stopwords("en"))
news_train$cleaned_text <- lapply(news_train$cleaned_text, tokenize_word_stems)
news_train$text_word_length <- lapply(news_train$cleaned_text, length)

# write.csv(sentiment(get_sentences(paste(news_train$cleaned_text, sep='', collapse=NULL)))$sentiment, 'C:\\Users\\Riley\\Documents\\Homework\\Ross\\TO_414\\to414-final-project\\train_sentiment1.csv')

# Returns emotional sentiment of article with numeric score
news_train$text_sentiment <- sentiment_train$sentiment_score
```

```{r}
names(news_train)[names(news_train)== "label"] <- "fake" #renaming dependent variable for readability purposes
```

### Test Data
```{r cache = TRUE}
# Cleaning article titles
news_test$cleaned_title <- tolower(news_test$title)
news_test$cleaned_title <- removePunctuation(news_test$cleaned_title)
news_test$cleaned_title <- stripWhitespace(news_test$cleaned_title)

news_test$title_character_length <- lapply(news_test$cleaned_title, nchar)#Creates variable of length of title
news_test$cleaned_title <- tokenize_words(news_test$cleaned_title, stopwords = stopwords::stopwords("en"))
news_test$cleaned_title <- lapply(news_test$cleaned_title, tokenize_word_stems)
news_test$title_word_length <- lapply(news_test$cleaned_title, length)
```


```{r cache = TRUE}
# Cleaning article text
news_test$cleaned_text <- tolower(news_test$text)
news_test$cleaned_text <- removePunctuation(news_test$cleaned_text)
news_test$cleaned_text <- stripWhitespace(news_test$cleaned_text)
news_test$text_character_length <- lapply(news_test$cleaned_text, nchar)
news_test$cleaned_text <- tokenize_words(news_test$cleaned_text, stopwords = stopwords::stopwords("en"))
news_test$cleaned_text <- lapply(news_test$cleaned_text, tokenize_word_stems)
news_test$text_word_length <- lapply(news_test$cleaned_text, length)

# write.csv(sentiment(get_sentences(paste(news_test$cleaned_text, sep='', collapse=NULL)))$sentiment, 'C:\\Users\\Riley\\Documents\\Homework\\Ross\\TO_414\\to414-final-project\\test_sentiment1.csv')

# Returns emotional sentiment of article with numeric score
news_test$text_sentiment <- sentiment_test$sentiment_score
```

```{r cache = TRUE}
# Adjusting structure types for usability 
news_train$title_character_length <- as.numeric(news_train$title_character_length)
news_train$text_character_length <- as.numeric(news_train$text_character_length)
news_train$title_word_length <- as.numeric(news_train$title_word_length)
news_train$text_word_length <- as.numeric(news_train$text_word_length)
news_train$text_sentiment <- as.numeric(news_train$text_sentiment)

news_test$title_character_length <- as.numeric(news_test$title_character_length)
news_test$text_character_length <- as.numeric(news_test$text_character_length)
news_test$title_word_length <- as.numeric(news_test$title_word_length)
news_test$text_word_length <- as.numeric(news_test$text_word_length)
news_test$text_sentiment <- as.numeric(news_test$text_sentiment)

```


## Summary of Varibles
```{r}
summary(news_train$fake)
summary(news_train$text_character_length)
summary(news_train$text_word_length)
summary(news_train$title_character_length)
summary(news_train$title_word_length)
summary(news_train$text_sentiment)

```

The Dependent Variable we are hoping to predict is whether or not an article is "fake" news, this is a binomial variable.

The independent variables we are using to predict fake news are: text_character_length (length of the article in characters), text_word_length (length of article in words), title_character_length (length of title in characters), title_word_length (length of title in words), sentiment (positive /negative scale of sentiment of words)

The sentiment scores scale from -1 to 1 with -1 being extremely negative sentiment vise versa for positive. A score around 0 would be neutral. Note that with a mean of 0.98 and a median of 0.38, the distribution of sentiment is slightly positive skewing.


```{r}
head(news_train)
```

```{r}
head(news_test)
```
## Logistic Regression
### Model Design
```{r}
FakeLog <- glm(fake~ text_character_length + text_word_length + title_character_length + title_word_length + text_sentiment, data = news_train )
summary(FakeLog)
#Significant factors(at alpha <= .05): text_character_length, title_character_length, title_word_length, text_sentiment 
```

```{r}
FakeLog <- glm(fake ~ text_character_length + text_word_length + title_character_length + title_word_length+ text_sentiment +(text_character_length*text_word_length) + (title_character_length*title_word_length) +(text_sentiment * text_character_length)+(text_sentiment * title_character_length)+ +(text_sentiment * text_word_length)+(text_sentiment * title_word_length), data = news_train )
summary(FakeLog)
#Significant factors(at alpha <= .05): text_character_length, text_word_length, title_character_length, title_word_length,text_sentiment, title_character_length*title_word_length, text_character_length*text_sentiment + title_character_length*text_sentiment + text_word_length*text_sentiment
```
```{r}
FakeLog <- glm(fake  ~ text_character_length+ text_word_length+ title_character_length+ title_word_length+text_sentiment+ title_character_length*title_word_length+ text_character_length*text_sentiment + title_character_length*text_sentiment + text_word_length*text_sentiment + I(text_character_length ^2) + I(title_character_length ^2) +I(text_word_length ^2) +I(title_word_length ^2) +I(text_sentiment ^2), data = news_train)
summary(FakeLog)
#Significant factors(at alpha <= .05): text_character_length, text_word_length, title_character_length, title_word_length,text_sentiment, title_character_length*title_word_length, text_character_length*text_sentiment , title_character_length*text_sentiment , text_word_length*text_sentiment , I(text_character_length ^2) , I(title_character_length ^2) ,I(text_word_length ^2) ,I(title_word_length ^2) ,I(text_sentiment ^2)
```
### Model Testing
```{r}
LogPredict <- predict(FakeLog, news_test)
LogPredict <- ifelse(LogPredict > 0.43, 1,0) #Tested multiple cutoffs, this cutoff produces highest Kappa
LogPredict <- as.factor(LogPredict)

confusionMatrix(news_test$fake, LogPredict)
```

### Analysis of Model
A few interesting takeaways come from our first model, logistic regression. First, all of the independent variables we made (text_character_length, text_word_length, title_character_length, title_word_length,text_sentiment) are significant in predicting if the news article is FAKE or not. Additionally, almost every interaction and the square values of each variable (excludes text_character_length:text_word_length). 

Our log model appears to be a moderately good predictor of if the article is fake news or not, suggested by the Kappa score of 0.4295. Hopefully the following models below will produce results as good or better than the log regression.

Note we did not create a linear regression due to our dependent variables being binomial. 

## Decision Tree

```{r}
#building decision tree model
library(C50)

news_train$fake <- as.factor(news_train$fake)

fakedecisionmodel <- C5.0(fake ~ text_character_length + text_word_length + title_character_length + title_word_length + text_sentiment , data = news_train)

plot(fakedecisionmodel)

summary(fakedecisionmodel)
```

```{r}
#checking prediction accuracy

fakepred <- predict(fakedecisionmodel, news_test)

library(gmodels)

CrossTable(news_test$fake, fakepred)

# Confusion Matrix
confusionMatrix(as.factor(fakepred), news_test$fake, positive = '1')
# 70.81% Accuracy with 0.4111 Kappa Statistics
```

```{r}
#adding error costs for false negative (missing identifying something as fake news)

error_cost <- matrix(c(0,0,3,0), nrow = 2)

error_cost

fakecostdecisionmodel <- C5.0(fake ~ text_character_length + text_word_length + title_character_length + title_word_length + text_sentiment, data = news_train, costs = error_cost)

plot(fakecostdecisionmodel)

summary(fakecostdecisionmodel)

```

```{r}
#checking new error cost model prediction accuracy

fakecostpred <- predict(fakecostdecisionmodel, news_test)

library(gmodels)

CrossTable(news_test$fake, fakecostpred)

#Confusion Matrix
confusionMatrix(as.factor(fakecostpred), as.factor(news_test$fake), positive = '1')
# Accuracy increases to 76% with improved Kappa Statistics of 0.4931
# Overall performance has been improved

```

### Analysis of Decision Tree Model

The first model proved to have a 70.81% accuracy with a 0.4111 Kappa Statistic. We improved the model by adding error cost. We assigned error to the model missing identifying news that is fake (false negative). After this improvement, the false negatives dropped from 778 to 67 with overall model accuracy improving to 76% with a 0.4931 Kappa Statistic.

The attribute usage is as followed:

	100.00%	title_word_length
	 68.10%	text_character_length
	 68.10%	title_character_length
	 30.84%	text_sentiment
	 21.50%	text_word_length
	 
This is consistent with the logistic regression model which saw title word length with the highest magnitude impact and text word length as the lowest magnitude impact on the predicted outcome.

## KNN Model

```{r}
library(class)
library(caret)

sent_train_knn <- sentiment_train
sent_test_knn <- sentiment_test
sent_train_labels <- news_train$fake
sent_test_labels <- news_test$fake

sqrt(nrow(sentiment_train)) 

# k = 143 -> accuracy of 69.31 and kappa of 0.3888
# k = 145 -> accuracy of 69.38 and kappa of 0.39

sent_test_pred <- knn(train = sent_train_knn, test = sent_test_knn,
                      cl = sent_train_labels, k=145)

#Evaluate model results
CrossTable(x = sent_test_labels, y = sent_test_pred, prop.chisq=FALSE)
confusionMatrix(sent_test_labels, sent_test_pred)
```

## ANN Model

```{r cache = TRUE}
sent_train_ann <- sentiment_train
sent_train_ann$fake <- news_train$fake
sent_train_ann$fake <- as.numeric(sent_train_ann$fake)
sent_train_ann$title_word <- news_train$title_word_length
sent_train_ann$text_word <- news_train$text_word_length
sent_train_ann$title_char <- news_train$title_character_length
sent_train_ann$text_char <- news_train$text_character_length

sent_test_ann <- sentiment_test
sent_test_ann$fake <- news_test$fake
sent_test_ann$fake <- as.numeric(sent_test_ann$fake)
sent_test_ann$title_word <- news_test$title_word_length
sent_test_ann$text_word <- news_test$text_word_length
sent_test_ann$title_char <- news_test$title_character_length
sent_test_ann$text_char <- news_test$text_character_length

normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

#Applying normalization function to entire dataframe
sent_train_ann <- as.data.frame(lapply(sent_train_ann, normalize))
sent_test_ann <- as.data.frame(lapply(sent_test_ann, normalize))


library(neuralnet)
ann_model <- neuralnet(formula = fake ~ ., data = sent_train_ann)

# visualize the network topology
plot(ann_model)

# obtain model results
results <- compute(ann_model, sent_test_ann)
# obtain predicted strength values
ann_results <- results$net.result
sent_ann_predict <- ifelse(ann_results>0.5,1,0) 
sent_ann_predict <- as.factor(sent_ann_predict)

CrossTable(x = sent_test_labels, y = sent_ann_predict, prop.chisq=FALSE)
confusionMatrix(sent_test_labels, sent_ann_predict)
```

```{r}
# Get a list of all the fake news
fake_news <- news_train[news_train$fake == 1, ]

# Use a function to get a list of everything (can't just c(news_train$cleaned_title)
#test <- do.call(c, news_train$cleaned_title)
test <- list(news_train$cleaned_title)
test <- unlist(test)

# Get rid of all one characters words
test <- test[lapply(X = test, FUN = nchar) != 1]

# Get rid of NULL values
test <- test[!sapply(test,is.null)]

# Dataframe of the words
words <- data.frame(matrix(unlist(test)))
colnames(words) <- "word"

# Sorted by frequency of words, top 20
freq_words <- count(words, word)
head(freq_words)
sorted_words <- freq_words[order(-freq_words$n),]
top20 <- head(sorted_words, 20)
head(top20)
#topwords <- list(top20$"matrix.unlist.test..")


# Check if the the words are in the title
news_train$has_word <- ifelse(top20$word %in% news_train$cleaned_title, 1, 0)

# Compare data to make sure whether it's working
# confusionMatrix(as.factor(news_train$has_word), as.factor(news_train$fake))
```

