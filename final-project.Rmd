---
title: "TO414 Final Project"
author: "Grace Chang, Seungwan Kim, Riley C Maher, Sage O'Toole, Jenna Kay Probst"
date: "4/27/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Introduction
Fake news is one of the biggest problems in the digital age, providing false information to our society. In-depth understandings with various approaches (considering subjective indicators) are needed to respond to fake news.  In other words, judging fake news cannot be done only by objective indicators. Fake news generally contains specific characteristics to attract attention to maximize advertising revenue, while it seems similar to credible journalism. 

Fake news can cause many business problems, as it reaches many ordinary readers who cannot discern it from the journalism. Therefore, these predictive models are significantly essential in preventing many social problems caused by fake news. Various linguistic features such as letters, natural languages led AI to distract in predict whether the information is fake or not. Therefore, we added some predictor variables to build reliable prediction models based on clues that fake news has in common.
Our report will include logistic regression models, decision tree model, KNN model, ANN model, and combined model. To improve the model's performance, we created the sentiment score and top 20 words to the given data. We believe that the two variables will allow our models to perform better.

# Fake News Prediction
A data set of article texts and corresponding labels if it is fake news were used to create models to try to predict if an article is fake news. We created a logistic regression model, decision tree model, KNN model, ANN model, and combined models to try to find the best way to predict fake news.

```{r}
library(tm)
library(stopwords)
library(tokenizers)
library(dplyr)
library(tidytext)
library(sentimentr)
library(gmodels)
library(stringi)
library(C50)
library(class)
library(caret)
library(neuralnet)
library('VIF')
```

## Loading Data
```{r}
# News Train
news_train <- read.csv('train.csv')
news_train$id <- NULL

news_test <- read.csv('test.csv')
news_test$id <- NULL

news_test_result <- read.csv('submit.csv')

head(news_train)
```

```{r}
sentiment_train <- read.csv('train_sentiment1.csv')
sentiment_test <- read.csv('test_sentiment1.csv')
```


```{r}
news_test$fake <- as.factor(news_test_result$label) #changed to factor in order to be used in confusion Matrix
```

## Data Preparation and Cleaning

### Train Data
```{r }
library(tokenizers)
library(stringi)
# Cleaning article titles
news_train$cleaned_title <- tolower(news_train$title)
news_train$cleaned_title <- stri_trans_general(news_train$title, "latin-ascii") # Remove/replace non-ascii characters
news_train$cleaned_title <- removePunctuation(news_train$cleaned_title)
news_train$cleaned_title <- stripWhitespace(news_train$cleaned_title)
# news_train$cleaned_title <- lapply(news_train$cleaned_title, tokenize_word_stems)

news_train$title_character_length <- lapply(news_train$cleaned_title, nchar) # Creates variable of length of title
news_train$cleaned_title <- tokenize_word_stems(news_train$cleaned_title, stopwords = stopwords::stopwords("en"))

news_train$title_word_length <- lapply(news_train$cleaned_title, length)

```


```{r }
# Cleaning article text
news_train$cleaned_text <- tolower(news_train$text)
news_train$cleaned_text <- removePunctuation(news_train$cleaned_text)
news_train$cleaned_text <- stripWhitespace(news_train$cleaned_text)

news_train$text_character_length <- lapply(news_train$cleaned_text, nchar)

news_train$cleaned_text <- tokenize_word_stems(news_train$cleaned_text, stopwords = stopwords::stopwords("en"))

news_train$text_word_length <- lapply(news_train$cleaned_text, length)

# write.csv(sentiment(get_sentences(paste(news_train$cleaned_text, sep='', collapse=NULL)))$sentiment, 'C:\\Users\\Riley\\Documents\\Homework\\Ross\\TO_414\\to414-final-project\\train_sentiment1.csv')

# Returns emotional sentiment of article with numeric score
news_train$text_sentiment <- sentiment_train$sentiment_score
```

```{r}
names(news_train)[names(news_train)== "label"] <- "fake" #renaming dependent variable for readability purposes
```

### Test Data
```{r cache = TRUE}
# Cleaning article titles
news_test$cleaned_title <- tolower(news_test$title)
news_test$cleaned_title <- removePunctuation(news_test$cleaned_title)
news_test$cleaned_title <- stripWhitespace(news_test$cleaned_title)
#news_test$cleaned_title <- lapply(news_test$cleaned_title, tokenize_word_stems)


news_test$title_character_length <- lapply(news_test$cleaned_title, nchar)#Creates variable of length of title

news_test$cleaned_title <- tokenize_words(news_test$cleaned_title, stopwords = stopwords::stopwords("en"))

news_test$title_word_length <- lapply(news_test$cleaned_title, length)
```


```{r}
# Cleaning article text
news_test$cleaned_text <- tolower(news_test$text)
news_test$cleaned_text <- removePunctuation(news_test$cleaned_text)
news_test$cleaned_text <- stripWhitespace(news_test$cleaned_text)

news_test$text_character_length <- lapply(news_test$cleaned_text, nchar)

news_test$cleaned_text <- tokenize_words(news_test$cleaned_text, stopwords = stopwords::stopwords("en"))

news_test$text_word_length <- lapply(news_test$cleaned_text, length)

# write.csv(sentiment(get_sentences(paste(news_test$cleaned_text, sep='', collapse=NULL)))$sentiment, 'C:\\Users\\Riley\\Documents\\Homework\\Ross\\TO_414\\to414-final-project\\test_sentiment1.csv')

# Returns emotional sentiment of article with numeric score
news_test$text_sentiment <- sentiment_test$sentiment_score
```

```{r}
# Adjusting structure types for usability 
news_train$title_character_length <- as.numeric(news_train$title_character_length)
news_train$text_character_length <- as.numeric(news_train$text_character_length)
news_train$title_word_length <- as.numeric(news_train$title_word_length)
news_train$text_word_length <- as.numeric(news_train$text_word_length)
news_train$text_sentiment <- as.numeric(news_train$text_sentiment)

news_test$title_character_length <- as.numeric(news_test$title_character_length)
news_test$text_character_length <- as.numeric(news_test$text_character_length)
news_test$title_word_length <- as.numeric(news_test$title_word_length)
news_test$text_word_length <- as.numeric(news_test$text_word_length)
news_test$text_sentiment <- as.numeric(news_test$text_sentiment)

```


## Summary of Varibles
```{r}
summary(news_train$fake)
summary(news_train$text_character_length)
summary(news_train$text_word_length)
summary(news_train$title_character_length)
summary(news_train$title_word_length)
summary(news_train$text_sentiment)

table(news_train$fake)
```

The Dependent Variable we are hoping to predict is whether or not an article is "fake" news, this is a binomial variable.

The independent variables we are using to predict fake news are: text_character_length (length of the article in characters), text_word_length (length of article in words), title_character_length (length of title in characters), title_word_length (length of title in words), sentiment (positive /negative scale of sentiment of words)

The sentiment scores scale from -1 to 1 with -1 being extremely negative sentiment vise versa for positive. A score around 0 would be neutral. Note that with a mean of 0.98 and a median of 0.38, the distribution of sentiment is slightly positive skewing.

## Logistic Regression
### Model Design
```{r}
FakeLog <- glm(fake ~ text_character_length + text_word_length + title_character_length + title_word_length + text_sentiment, data = news_train )
summary(FakeLog)
#Significant factors(at alpha <= .05): text_character_length, text_word_length, title_character_length, title_word_length, text_sentiment 
```

```{r}
FakeLog <- glm(fake ~ text_character_length + text_word_length + title_character_length + title_word_length+ text_sentiment +(text_character_length*text_word_length) + (title_character_length*title_word_length) +(text_sentiment * text_character_length)+(text_sentiment * title_character_length)+ +(text_sentiment * text_word_length)+(text_sentiment * title_word_length), data = news_train )
summary(FakeLog)
#Significant factors(at alpha <= .05): text_character_length, text_word_length, title_word_length,text_sentiment, text_character_length*text_word_length, title_character_length*title_word_length, text_character_length*text_sentiment + title_character_length*text_sentiment + text_word_length*text_sentiment
```
```{r}
FakeLog <- glm(fake  ~ text_character_length+ text_word_length+ title_character_length+ title_word_length+text_sentiment+ title_character_length*title_word_length+ text_character_length*text_sentiment + title_character_length*text_sentiment + text_word_length*text_sentiment + I(text_character_length ^2) + I(title_character_length ^2) +I(text_word_length ^2) +I(title_word_length ^2) +I(text_sentiment ^2), data = news_train)
summary(FakeLog)
#Significant factors(at alpha <= .05): text_character_length, text_word_length, title_character_length, title_word_length,text_sentiment, title_character_length*title_word_length, text_character_length*text_sentiment , title_character_length*text_sentiment , text_word_length*text_sentiment , I(text_character_length ^2) , I(title_character_length ^2) ,I(text_word_length ^2) ,I(title_word_length ^2) ,I(text_sentiment ^2)
```
### Model Testing
```{r}
LogPredict <- predict(FakeLog, news_test)
LogPredict <- ifelse(LogPredict > 0.43, 1,0) #Tested multiple cutoffs, this cutoff produces highest Kappa
LogPredict <- as.factor(LogPredict)

confusionMatrix(news_test$fake, LogPredict)
```

### Analysis of Logistic Regression Model
A few interesting takeaways come from our first model, logistic regression. First, all of the independent variables we made (text_character_length, text_word_length, title_character_length, title_word_length,text_sentiment) are significant in predicting if the news article is FAKE or not. Additionally, almost every interaction and the square values of each variable (excludes text_character_length:text_word_length). 

Our log model appears to be a moderately good predictor of if the article is fake news or not, suggested by the Kappa score of 0.4295. Hopefully the following models below will produce results as good or better than the log regression.

Note we did not create a linear regression due to our dependent variables being binomial. 

## Decision Tree

### Model Design

```{r}
#building decision tree model
library(C50)

news_train$fake <- as.factor(news_train$fake)

fakedecisionmodel <- C5.0(fake ~ text_character_length + text_word_length + title_character_length + title_word_length + text_sentiment , data = news_train)

plot(fakedecisionmodel)

summary(fakedecisionmodel)
```
### Model Testing

```{r}
#checking prediction accuracy

fakepred <- predict(fakedecisionmodel, news_test)

library(gmodels)

CrossTable(news_test$fake, fakepred)

# Confusion Matrix
confusionMatrix(as.factor(fakepred), news_test$fake, positive = '1')
# 72.13% Accuracy with 0.4372 Kappa Statistics
```
### Adding Error Cost

```{r}
#adding error costs for false negative (missing identifying something as fake news)

error_cost <- matrix(c(0,0,3,0), nrow = 2)

error_cost

fakecostdecisionmodel <- C5.0(fake ~ text_character_length + text_word_length + title_character_length + title_word_length + text_sentiment, data = news_train, costs = error_cost)

plot(fakecostdecisionmodel)

summary(fakecostdecisionmodel)

```
### Model Testing
```{r}
#checking new error cost model prediction accuracy

fakecostpred <- predict(fakecostdecisionmodel, news_test)

CrossTable(news_test$fake, fakecostpred)

#Confusion Matrix
confusionMatrix(as.factor(fakecostpred), as.factor(news_test$fake), positive = '1')
# Accuracy increases to 78.13% with improved Kappa Statistics of 0.538
# Overall performance has been improved

```

### Analysis of Decision Tree Model

The first model proved to have a 72.13% accuracy with a 0.4372 Kappa Statistic. We improved the model by adding error cost. We assigned error cost to the model missing identifying news that is fake (false negative). This is because there is more cost to labeling an article as true when it is spreading fake news than labeling an article as fake news and fact checking an article that is actually true. After this improvement, the false negatives dropped from 729 to 8 with overall model accuracy improving to 78.13% with a 0.538 Kappa Statistic.

The attribute usage is as followed:

  100.00%	title_word_length
	 67.93%	text_character_length
	 32.63%	title_character_length
	 27.34%	text_sentiment
	 19.23%	text_word_length

This is consistent with the logistic regression model which saw title word length with the highest magnitude impact and text word length as the lowest magnitude impact on the predicted outcome.

## Top 20 Words in Fake News

### In Title

```{r}
# Get all the fake news rows
fake_news <- news_train[news_train$fake == 1, ] # Fake

# Use a function to get a list of everything (can't just c(news_train$cleaned_title)
title <- do.call(c, fake_news$cleaned_title)

# Get rid of all one characters words
title <- title[lapply(X = title, FUN = nchar) != 1]

# Get a list of the top 20 words (just the words)
top_title_words <- names(sort(table(title), decreasing = TRUE)[1:20])

# Check if the the words are in the title
news_train$title_has_word <- ifelse(top_title_words %in% news_train$cleaned_title, 1, 0)

# Apply to test data
news_test$title_has_word <- ifelse(top_title_words %in% news_test$cleaned_title, 1, 0)
```

### In Text

```{r}
# Get all the fake news rows
fake_news <- news_train[news_train$fake == 1, ]

# List of all words
text <- do.call(c, fake_news$cleaned_text)

# Get rid of all one characters words
text <- text[lapply(X = text, FUN = nchar) != 1]

# Get a list of the top 20 words (just the words)
top_text_words <- names(sort(table(text), decreasing = TRUE)[1:20])

# Check if the the words are in the text
news_train$text_has_word <- ifelse(top_text_words %in% news_train$cleaned_text, 1, 0)

# Apply to test data
news_test$text_has_word <- ifelse(top_text_words %in% news_test$cleaned_text, 1, 0)
```

## KNN Model

### Model 1
```{r}
# Train and Test
train_knn <- news_train[,6:13]
train_knn$cleaned_text <- NULL
test_knn <- news_test[,6:13]
test_knn$cleaned_text <- NULL
train_labels <- news_train$fake
test_labels <- news_test$fake

str(train_knn)
# Normalize
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

train_knn <- as.data.frame(lapply(train_knn, normalize))
test_knn <- as.data.frame(lapply(test_knn[1:6], normalize))
test_knn$text_has_word <- news_test$text_has_word

sqrt(nrow(train_knn)) 

knn_test_pred <- knn(train = train_knn, test = test_knn,
                      cl = train_labels, k=143)

#Evaluate model results
confusionMatrix(data = knn_test_pred, reference = test_labels)
# Accuracy of 0.5492 and kappa of 0.0583
```

### Model 2
```{r}
# KNN Model with sentiment and top20 words
train_knn <- sentiment_train
train_knn$title_has_word <- news_train$title_has_word
train_knn$text_has_word <- news_train$text_has_word

test_knn <- sentiment_test
test_knn$title_has_word <- news_test$title_has_word
test_knn$text_has_word <- news_test$text_has_word

train_knn <- as.data.frame(lapply(train_knn, normalize))
test_knn <- as.data.frame(lapply(test_knn, normalize))
test_knn$text_has_word <- news_test$text_has_word

knn_test_pred <- knn(train = train_knn, test = test_knn,
                      cl = train_labels, k=143)
confusionMatrix(data = knn_test_pred, reference = test_labels)
# Accuracy of 0.6876 and kappa of 0.3787
```

### Analysis of KNN Models

Model 1 had a 54.92% accuracy, kappa statistic of 0.0583 and 750 false negatives. Model 2 was better with an accuracy of 68.76% and kappa statistic of 0.3787, but had more false negatives with 1030. Since Model 1 has an extremely low kappa statistic, only Model 2 will be used in the combined model.

## ANN Model

```{r cache = TRUE}
train_ann <- train_knn
train_ann$fake <- news_train$fake
test_ann <- test_knn
test_ann$fake <- news_test$fake
train_ann$fake <- as.factor(train_ann$fake)

ann_model <- neuralnet(formula = fake ~ ., data = train_ann, linear.output = F, stepmax = 1000000)

# visualize the network topology
plot(ann_model)

# obtain model results
results <- compute(ann_model, test_ann)
# obtain predicted strength values
ann_results <- results$net.result[,2]
ann_predict <- ifelse(ann_results>0.5,1,0) 
ann_predict <- as.factor(ann_predict)

confusionMatrix(data = ann_predict, reference = test_labels)
# Accuracy 0.6677 and kappa 0.394
```
### Analysis of ANN Model

This model uses the following variables: sentiment score, if the title has the common words in fake news titles, and if the text has the common words in fake news text. The model is 66.92% accurate, has a kappa statistic of 0.3124, and 499 false negatives.

## Combined Models

```{r}
#creating new data set using 4 models predictions and the data test real results
combinedpred <- data.frame(LogPredict, fakecostpred, knn_test_pred,  ann_predict, test_labels)

#making variables numeric
combinedpred$LogPredict <- ifelse(combinedpred$LogPredict == "1", 1,0)
combinedpred$fakecostpred <- ifelse(combinedpred$fakecostpred == "1", 1,0)
combinedpred$knn_test_pred <- ifelse(combinedpred$knn_test_pred == "1", 1,0)
combinedpred$ann_predict <- ifelse(combinedpred$ann_predict == "1", 1,0)
combinedpred$test_labels <- ifelse(combinedpred$test_labels == "1", 1,0)

#combining model predictions
combinedpred$predict <- combinedpred$LogPredict + combinedpred$fakecostpred + combinedpred$knn_test_pred + combinedpred$ann_predict

#creating binary prediction if 1 or more models predict article is fake news
combinedpred$predict1 <- ifelse(combinedpred$predict > 0, 1, 0)

#creating binary prediction if 2 or more models predict article is fake news
combinedpred$predict2 <- ifelse(combinedpred$predict > 1, 1, 0)

#creating binary prediction if 3 or more models predict article is fake news
combinedpred$predict3 <- ifelse(combinedpred$predict > 2, 1, 0)

#creating binary prediction if all 4 models predict article is fake news
combinedpred$predict4 <- ifelse(combinedpred$predict > 3, 1, 0)

#testing accuracy of combined models

confusionMatrix(as.factor(combinedpred$predict1), as.factor(combinedpred$test_labels))
```
positive prediction if 1 or more models predict article is fake news: 67.21% accuracy; 0.2906 kappa statistic; 4 false negatives
```{r}
confusionMatrix(as.factor(combinedpred$predict2), as.factor(combinedpred$test_labels))
```
positive prediction if 2 or more models predict article is fake news: 73.02% accuracy; 0.5502 kappa statistic; 228 false negatives
```{r}
confusionMatrix(as.factor(combinedpred$predict3), as.factor(combinedpred$test_labels))
```
positive prediction if 3 or more models predict article is fake news: 74.83% accuracy; 0.4936 kappa statistic; 710 false negatives
```{r}
confusionMatrix(as.factor(combinedpred$predict4), as.factor(combinedpred$test_labels))
```
positive prediction if all 4 models predict article is fake news: 69.56% accuracy; 0.4097 kappa statistic; 1329 false negatives

### Analysis of Combined Model
The combined model requiring 3 or more models to predict fake news had the highest accuracy of the combined models with 74.83%. The model only requiring 2 or more models predicting fake news had the highest kappa statistic of the combined models with 0.5502. The model only requiring 1 or more models to predict fake news (as can be expected) had the lowest number of false negatives with only 4.

## Conclusion
Advances in technology have allowed fake news to approach the public quickly. Fake information can bring a big wave of key events in societies, including presidential elections and stock price, meaning its risks and ripple effects are beyond our expectations. However, with the artificial intelligence prediction models, we would be able to control the spread of fake news more efficiently than traditional methods. Although decision tree models have shown the best performance among the things we have created in this project, the results would vary depending on the data and approaches to improve further models.

The decision tree model had the best prediction of all the models in terms of accuracy and kappa statistic with 78.13% and 0.538, respectively. This model had 8 false negatives. The only model with fewer false negatives was the combined model requiring only 1 of the 4 models to predict fake news to label the article as fake news. This combined model only has a 0.2906 kappa statistic, so overall the decision tree model is still the best model to predict if articles are fake news. 

The limitation of our prediction model was not only having irregular in the given data but also contained many errors. Those could negatively affect the performance of all types of predictive models.
For future improvements, we would consider adding more key predictor variables, such as the combination of words most commonly used in fake news (n-gram) and using LSTM.

Our team's submission score by kaggle was 0.63397, but we believe this is a great first step of distinguishing fake news. 